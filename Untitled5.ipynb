{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89781f8c-b952-4207-8bbb-a298f0f11045",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from decouple import config\n",
    "url = \"https://api.gladia.io/v2/transcription\"\n",
    "\n",
    "payload = {\n",
    "    \"context_prompt\": \"You are being given a YouTube video. Determine the target audience and write a summary that would appeal to that audience\",\n",
    "    \n",
    "    \"detect_language\": True,\n",
    "    \"summarization\": True,\n",
    "    \"display_mode\": True,\n",
    "    \"audio_url\": \"https://www.youtube.com/watch?v=1CGlGcQMF8Y\"\n",
    "}\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    'x-gladia-key': config('GLADIA_API_KEY'),\n",
    "\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe5f9a-711c-470b-84b2-622b62c37bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url, json=payload, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a49be8-1a81-47a7-9153-ae93977457f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600aae46-d41d-4515-9705-1ee1d3eeca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.gladia.io/v2/transcription/5520e97a-0f67-4c99-b45f-0cf7094a3099\"\n",
    "\n",
    "headers = {\"x-gladia-key\": config('GLADIA_API_KEY')}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a463162-66e5-4856-80bd-611f2cb5bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "def make_fetch_request(url, headers, method='GET', data=None):\n",
    "    if method == 'POST':\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "    else:\n",
    "        response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "gladia_key = config('GLADIA_API_KEY')\n",
    "request_data = {\n",
    "    \"audio_url\": \"https://www.youtube.com/watch?v=1CGlGcQMF8Y\",\n",
    "    \"diarization\": True,\n",
    "    \n",
    "}\n",
    "gladia_url = \"https://api.gladia.io/v2/transcription/\"\n",
    "\n",
    "headers = {\n",
    "    \"x-gladia-key\": gladia_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(\"- Sending initial request to Gladia API...\")\n",
    "initial_response = make_fetch_request(\n",
    "    gladia_url, headers, 'POST', request_data)\n",
    "\n",
    "print(\"Initial response with Transcription ID:\", initial_response)\n",
    "result_url = initial_response.get(\"result_url\")\n",
    "\n",
    "if result_url:\n",
    "    while True:\n",
    "        print(\"Polling for results...\")\n",
    "        poll_response = make_fetch_request(result_url, headers)\n",
    "\n",
    "        if poll_response.get(\"status\") == \"done\":\n",
    "            print(\"- Transcription done: \\n\")\n",
    "            summarization = poll_response.get(\"result\", {}).get(\n",
    "                \"summarization\", {})\n",
    "            print(summarization)\n",
    "            break\n",
    "        else:\n",
    "            print(\"Transcription status:\", poll_response.get(\"status\"))\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df1b30-97cf-4367-892a-4c7c94cd4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4044c-9427-4fe3-8cf4-43fc8838adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "channel_id = \"UC_Mn3zTIipnTQ8Zy_QtXZew\"\n",
    "\n",
    "rss_url=f\"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}\"\n",
    "d = feedparser.parse(rss_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc684eb-411d-4c4c-8bfb-ab48b68d0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = d['entries']\n",
    "print(len(entries))\n",
    "entries_not_shorts = []\n",
    "for entry in entries:\n",
    "    if '#shorts' not in entry['title']:\n",
    "        entries_not_shorts.append(entry)\n",
    "print(len(entries_not_shorts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868c850-c588-44b1-80a7-d7a17aebd732",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06afc5c-3563-4c68-bd52-5852cdd7294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens  {\n",
    "\n",
    "# }\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "\n",
    "def list_channel_videos(credentials: Credentials):\n",
    "    # Build the YouTube API client\n",
    "    youtube = build('youtube', 'v3', credentials=credentials)\n",
    "\n",
    "    # Get the channel ID of the authenticated user\n",
    "    channels_response = youtube.channels().list(\n",
    "        part='id',\n",
    "        mine=True\n",
    "    ).execute()\n",
    "\n",
    "    if not channels_response['items']:\n",
    "        print(\"No channels found for the authenticated user.\")\n",
    "        return\n",
    "\n",
    "    channel_id = channels_response['items'][0]['id']\n",
    "\n",
    "    # Get the uploads playlist ID for the channel\n",
    "    playlists_response = youtube.channels().list(\n",
    "        part='contentDetails',\n",
    "        id=channel_id\n",
    "    ).execute()\n",
    "\n",
    "    uploads_playlist_id = playlists_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "    # Retrieve the list of videos in the uploads playlist\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        playlistitems_response = youtube.playlistItems().list(\n",
    "            part='snippet',\n",
    "            playlistId=uploads_playlist_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        for item in playlistitems_response['items']:\n",
    "            videos.append(item['snippet']['title'])\n",
    "\n",
    "        next_page_token = playlistitems_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return videos\n",
    "\n",
    "def list_channel_videos_excluding_shorts(credentials: Credentials):\n",
    "    # Build the YouTube API client\n",
    "    youtube = build('youtube', 'v3', credentials=credentials)\n",
    "\n",
    "    # Get the channel ID of the authenticated user\n",
    "    channels_response = youtube.channels().list(\n",
    "        part='id',\n",
    "        mine=True\n",
    "    ).execute()\n",
    "\n",
    "    if not channels_response['items']:\n",
    "        print(\"No channels found for the authenticated user.\")\n",
    "        return\n",
    "\n",
    "    channel_id = channels_response['items'][0]['id']\n",
    "\n",
    "    # Get the uploads playlist ID for the channel\n",
    "    playlists_response = youtube.channels().list(\n",
    "        part='contentDetails',\n",
    "        id=channel_id\n",
    "    ).execute()\n",
    "\n",
    "    uploads_playlist_id = playlists_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "    # Retrieve the list of videos in the uploads playlist\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        playlistitems_response = youtube.playlistItems().list(\n",
    "            part='snippet',\n",
    "            playlistId=uploads_playlist_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        video_ids = [item['snippet']['resourceId']['videoId'] for item in playlistitems_response['items']]\n",
    "\n",
    "        # Get video details to filter out Shorts\n",
    "        videos_response = youtube.videos().list(\n",
    "            part='contentDetails',\n",
    "            id=','.join(video_ids)\n",
    "        ).execute()\n",
    "\n",
    "        for item in videos_response['items']:\n",
    "            duration = item['contentDetails']['duration']\n",
    "            if 'S' not in duration or ('H' in duration or 'M' in duration):\n",
    "                # Exclude videos with only seconds in duration (Shorts)\n",
    "                videos.append(item['id'])\n",
    "\n",
    "        next_page_token = playlistitems_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return videos\n",
    "\n",
    "\n",
    "#   access_token: '',\n",
    "#   refresh_token: '',\n",
    "#   scope: 'https://www.googleapis.com/auth/youtubepartner',\n",
    "#   token_type: 'Bearer',\n",
    "#   expiry_date: 1717630108432\n",
    "# Example credentials, replace with your own\n",
    "credentials = Credentials.from_authorized_user_info({\n",
    "    'token': '',\n",
    "    'refresh_token': '',\n",
    "    'token_uri': 'https://oauth2.googleapis.com/token',\n",
    "    'client_id': '',\n",
    "    'client_secret': ''\n",
    "})\n",
    "\n",
    "videos = list_channel_videos(credentials)\n",
    "videos_without_shorts = list_channel_videos_excluding_shorts(credentials)\n",
    "\n",
    "print(len(videos))\n",
    "print(len(videos_without_shorts))\n",
    "# for video in videos:\n",
    "#     if '#shorts' not in video:\n",
    "#         print(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c06905-d2c6-492c-a390-345d504acead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_pinecone  import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "import uuid \n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "\n",
    "\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "embeddings = OpenAIEmbeddings(\n",
    "                            model=\"text-embedding-3-small\",\n",
    "                            openai_api_key=config(\"OPENAI_API_KEY\"),\n",
    "                            openai_api_base=config('OPENAI_API_BASE'),\n",
    "                            headers={\n",
    "                                \"Helicone-Auth\": f\"Bearer {config('HELICONE_API_KEY')}\"\n",
    "                            })\n",
    "index = pc.Index('prelovc', host=PINECONE_HOST)\n",
    "vectorstore = PineconeVectorStore(index, embeddings, \"text\")\n",
    "\n",
    "webpages = {}\n",
    "text_splitter = SemanticChunker(embeddings)\n",
    "backup_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "def add_page_to_vectorstore(webpage_text, webpage_name, webpage_url, namespace):\n",
    "    chunks = text_splitter.split_text(webpage_text)\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    chunks_to_save = []\n",
    "    webpage_id = str(uuid.uuid4())\n",
    "    webpages[namespace] = {\n",
    "        f'{webpage_id}': {\n",
    "            \"title\": webpage_name,\n",
    "            \"url\": webpage_url\n",
    "        }\n",
    "    }\n",
    "    for chunk in chunks:\n",
    "        chunk_id = str(uuid.uuid4())\n",
    "        metadata = {\n",
    "            \"webpage_id\": webpage_id,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"webpage_name\": webpage_name,\n",
    "            \"url\": webpage_url,\n",
    "            \"text\": chunk\n",
    "        }\n",
    "\n",
    "        # test metadata size\n",
    "        size = sys.getsizeof(json.dumps(metadata))\n",
    "\n",
    "        if size > 40960:\n",
    "            # remove the snippet, because it's too big\n",
    "            snippet.delete()\n",
    "            # need to split into smaller chunks\n",
    "            smaller_chunks = backup_text_splitter.split_text(chunk)\n",
    "            for smaller_chunk in smaller_chunks:\n",
    "                smaller_id = str(uuid.uuid4())\n",
    "                \n",
    "                smaller_metadata = {\n",
    "                         \"webpage_id\": webpage_id,\n",
    "                        \"chunk_id\": smaller_id,\n",
    "                        \"webpage_name\": webpage_name,\n",
    "                        \"url\": webpage_url,\n",
    "                        \"text\": smaller_chunk\n",
    "                }\n",
    "                metadatas.append(smaller_metadata)\n",
    "                ids.append(smaller_id)\n",
    "                chunks_to_save.append(smaller_chunk)\n",
    "            continue\n",
    "        else:\n",
    "            chunks_to_save.append(chunk)\n",
    "            metadatas.append(metadata)\n",
    "            ids.append(chunk_id)\n",
    "\n",
    "    vectorstore.add_texts(chunks_to_save, metadatas=metadatas, ids=ids, namespace=namespace)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879e4bb-2d36-416f-b145-74a85a6937c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "links_to_process = []\n",
    "\n",
    "\n",
    "def url_to_filename(url):\n",
    "    safe_name = re.sub(r\"[^\\w\\-_]\", \"_\", url)\n",
    "    return safe_name\n",
    "\n",
    "def clean_string(input_string):\n",
    "    cleaned_string = \"\".join(char for char in input_string.lower() if char.isalpha())\n",
    "    return cleaned_string\n",
    "\n",
    "def download_file(download_url, file_name):\n",
    "    req = Request(download_url, headers={'User-Agent' : \"Magic Browser\"})\n",
    "\n",
    "    response = urlopen(req)\n",
    "    f = open(file_name, 'wb')\n",
    "    f.write(response.read())\n",
    "    f.close()\n",
    "\n",
    "def process_page(soup: BeautifulSoup, url, namespace):\n",
    "    # Placeholder function\n",
    "    # Add your scraping code here\n",
    "    title = soup.title.string if soup.title else url\n",
    "    content = soup.get_text(strip=True)\n",
    "    print(f\"----- Processing page: {soup.title.string} -----\")\n",
    "    print(content)\n",
    "    file_name = url_to_filename(url)\n",
    "    download_file(url, file_name)\n",
    "    loader = BSHTMLLoader(file_name)\n",
    "\n",
    "    data = loader.load()\n",
    "\n",
    "    content = soup.get_text(strip=True)\n",
    "\n",
    "    add_page_to_vectorstore(content, title, url, namespace)\n",
    "       \n",
    "\n",
    "\n",
    "def is_valid(url: str) -> bool:\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_all_links(url: str, root_url: str):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0.1'}\n",
    "    source_code = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(source_code.content, 'html.parser')\n",
    "\n",
    "    for link in soup.find_all('a'):\n",
    "        link_href = link.get('href')\n",
    "        if link_href:\n",
    "            absolute_url = urljoin(root_url, link_href)\n",
    "            if is_valid(absolute_url) and absolute_url.startswith(root_url):\n",
    "                yield absolute_url\n",
    "\n",
    "def crawl(base_url: str, namespace: str):\n",
    "    print(f\"----- Crawling: {base_url} -----\")\n",
    "\n",
    "    crawled_urls = set()\n",
    "    to_crawl = {base_url}\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.verify = False\n",
    "    while to_crawl:\n",
    "        url = to_crawl.pop()\n",
    "        try:\n",
    "            print(f\"----- Crawling: {url} -----\")\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0.1'}\n",
    "            response = session.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            process_page(soup, url, namespace)\n",
    "\n",
    "            crawled_urls.add(url)\n",
    "\n",
    "            for link in get_all_links(url, base_url):\n",
    "                if link not in crawled_urls:\n",
    "                    to_crawl.add(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {url}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9deb3ed-36b7-4cba-8e64-a7cceb8b8650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b8b723-9f75-4008-8711-e1a82a5bb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = 'INVESTOR_WEBPAGES'\n",
    "crawl('https://www.hustlefund.vc/', namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3343e316-a9dc-4247-87e6-64fc5d515155",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = 'RedbudVC'\n",
    "crawl('https://www.redbud.vc/', namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fe487-3365-4f6a-b8dd-3982277e431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = 'Fenway Summer'\n",
    "crawl('https://www.fenwaysummer.com/', namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdbff7d-9601-4624-8d57-c1bf9e1ea5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = vectorstore.similarity_search_with_score(\"What's the thesis of HustleFund?\", k=10, namespace='INVESTOR_WEBPAGES')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c172c5-e487-40aa-9034-e0c6d48b95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = 'Fenway Summer'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553acbe7-ce3a-4de6-828f-026e5e9e4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What industries do you focus on?\n",
    "# What stage of startups do you invest in?\n",
    "# What is your typical check size?\n",
    "# What are your key investment criteria?\n",
    "# How do you evaluate market potential?\n",
    "# What is your approach to risk management?\n",
    "# How involved are you post-investment?\n",
    "# What is your expected timeline for returns?\n",
    "# Can you share examples of successful investments?\n",
    "# What are your views on current market trends?\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from submind.models import Submind\n",
    "from submind.memory.memory import remember\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from submind.llms.submind import SubmindModelFactory\n",
    "from decouple import config\n",
    "submind = Submind.objects.get(id=config(\"PRELO_SUBMIND_ID\"))\n",
    "submind_document = remember(submind)\n",
    "vectorstore_to_search = PineconeVectorStore(index, embeddings, \"text\", namespace=namespace)\n",
    "retriever = vectorstore_to_search.as_retriever()\n",
    "model = SubmindModelFactory.get_model('fenwaysummer', \"learn_thesis\")\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for answering questions about {investor}'s thesis. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    model, retriever, contextualize_q_prompt\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34876e4f-a4ba-409b-a700-e6259993374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n",
    "questions_to_ask = [\n",
    "    'What industries does Hustle Fund focus on?',\n",
    "    'What stage of startups does Hustle Fund invest in?',\n",
    "    \"What is Hustle Fund's typical check size?\",\n",
    "    \"What are Hustle Fund's key investment criteria?\",\n",
    "    'How does Hustle Fund evaluate market potential?',\n",
    "    \"What is Hustle Fund's approach to risk management?\",\n",
    "    'How involved are Hustle Fund post-investment?',\n",
    "    \"What is Hustle Fund's expected timeline for returns?\",\n",
    "    \"Can you share examples of Hustle Fund's successful investments?\",\n",
    "    \"What are Hustle Fund's views on current market trends?\"\n",
    "]\n",
    "answers = []\n",
    "\n",
    "for question in questions_to_ask:\n",
    "    answers.append(conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"abc123\"}\n",
    "        },  # constructs a key \"abc123\" in `store`.\n",
    "    )[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a1e6f6-b01e-4687-b509-2b45a9a6d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c1f40-c42d-4f58-be59-16c47f2dbbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "SUBMIND_PROMPT = \"\"\"\n",
    "You are a powerful submind for a top early-stage investor.\n",
    "\n",
    "Here's what you know about early-stage investing: {mind}\n",
    "\n",
    "If a founder wanted to know more about your thesis, what 5 questions should they ask?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = SubmindModelFactory.get_model('investor_thesis', \"get_questions\")\n",
    "prompt = ChatPromptTemplate.from_template(SUBMIND_PROMPT)\n",
    "chain = prompt | model | StrOutputParser()\n",
    "submind = Submind.objects.get(id=config(\"PRELO_SUBMIND_ID\"))\n",
    "submind_document = remember(submind)\n",
    "response = chain.invoke({\n",
    "    \"mind\": submind_document,\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407d415d-c68a-4ce5-8721-30f7c087cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_thesis = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e3c9d-76fb-429a-81d4-8a0fcee5a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_THESIS_PROMPT = \"\"\"\n",
    "You are a powerful submind for an early stage investor.\n",
    "\n",
    "Here's what you know about investing: {mind}\n",
    "\n",
    "You are studying another investor and trying to learn their thesis.\n",
    "\n",
    "Here's what you know about their thesis so far: {current_thesis}\n",
    "\n",
    "Here is what you are looking for to understand their thesis:\n",
    "What specific industries or sectors do you focus on for early-stage investments?\n",
    "What key metrics or indicators do you prioritize when evaluating early-stage startups?\n",
    "How do you assess the potential for scalability and market growth in a startup?\n",
    "What level of involvement do you typically have with your portfolio companies, and what resources or support do you offer?\n",
    "What are your expectations for exit strategies, and how do you evaluate potential exit opportunities?\n",
    "\n",
    "What's the next question you would ask in order to better understand the investor's thesis? Only ask a single question.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "next_question_model = SubmindModelFactory.get_model('investor_thesis', \"next_question\")\n",
    "next_question_prompt = ChatPromptTemplate.from_template(LEARN_THESIS_PROMPT)\n",
    "next_question_chain = next_question_prompt | next_question_model | StrOutputParser()\n",
    "next_question = next_question_chain.invoke({\n",
    "    \"mind\": submind_document,\n",
    "    \"current_thesis\": current_thesis\n",
    "})\n",
    "print(next_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f6f56-6216-4511-862f-109f9ecf4e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n",
    "answer = conversational_rag_chain.invoke(\n",
    "    {\"input\": next_question,\n",
    "    \"investor\": \"RedbudVC\"\n",
    "    },\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"abc123\"}\n",
    "        },  # constructs a key \"abc123\" in `store`.\n",
    "    )[\"answer\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88f5b44-0acb-419a-ae8b-e0b1896ace82",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = 'They are industry and sector agnostic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132f6f3-68c1-4a97-ae39-da6dfa96be56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7074cb3-53fd-49b5-89e0-2053bcabea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECT_THESIS_PROMPT=\"\"\"\n",
    "You are a powerful submind for an early stage investor.\n",
    "\n",
    "Here's what you know about investing: {mind}\n",
    "\n",
    "You are studying another investor and trying to learn their thesis.\n",
    "\n",
    "Here's what you know about their thesis so far: {current_thesis}\n",
    "\n",
    "Here's the last question you asked: {question}\n",
    "\n",
    "Here's the answer you got: {answer}\n",
    "\n",
    "Based on that question and answer, revise your current understanding of their thesis.\n",
    "\"\"\"\n",
    "\n",
    "reflect_model = SubmindModelFactory.get_model('investor_thesis', \"reflect\")\n",
    "reflect_prompt = ChatPromptTemplate.from_template(REFLECT_THESIS_PROMPT)\n",
    "reflect_chain = reflect_prompt | reflect_model | StrOutputParser()\n",
    "new_thesis = reflect_chain.invoke({\n",
    "    \"mind\": submind_document,\n",
    "    \"current_thesis\": current_thesis,\n",
    "    \"question\": next_question,\n",
    "    \"answer\": answer\n",
    "})\n",
    "print(new_thesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94193d09-b1d1-4b9f-88ee-66d523849836",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_thesis = new_thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed849f4-09ed-41db-b466-65d89ea16419",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77dd01-7694-4e0c-a270-726e57d414be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = SubmindModelFactory.get_model('testing_retrieve', \"thesis\")\n",
    "\n",
    "question = \"You are a startup founder that is evaluating venture firms. What is the investment thesis of Fenway Summer?\"\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1d74f-b40e-4e1e-8928-8fb3a42cf982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd9609-3926-45e2-a271-c51153003116",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs = retriever_from_llm.invoke(question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf2132-5d8f-48ae-bd5c-b93f626443f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return lines\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Chain\n",
    "llm_chain = QUERY_PROMPT | llm | output_parser\n",
    "\n",
    "# Other inputs\n",
    "question = \"What are the approaches to Task Decomposition?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8ba2e-c4df-46bd-b7be-58177f5808ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "investor = \"Fenway Summer\"\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "THESIS_LOOKUP_PROMPT = \"\"\" You are a powerful submind that is an expert on early stage investing.\n",
    "\n",
    "Here's what you know about investing: {mind}\n",
    "\n",
    "You are learning about the investment thesis of {investor}.\n",
    "\"\"\"\n",
    "lookup_prompt = ChatPromptTemplate.from_template(THESIS_LOOKUP_PROMPT)\n",
    "\n",
    "context = format_docs(unique_docs)\n",
    "\n",
    "rag_chain = lookup_prompt| model | StrOutputParser()\n",
    "\n",
    "\n",
    "answer = rag_chain.invoke({\n",
    "     \"mind\": submind_document, \n",
    "    \"investor\": investor,\n",
    "    \"input\": \"What's their investment thesis?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741865ed-5bc4-4237-b7cb-3196e13f740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97503069-433d-44aa-834c-2f00e61d9d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create retriever for investor-specific namespace.\n",
    "# Step 2: Use Multiquery retriever with custom prompt to pull documents from the vectorstore that give the correct context.\n",
    "# Step 3: Pass those documents into a prompt as a context to answer the original question of what is the investor's thesis.\n",
    "# Step 4: Profit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Environment",
   "language": "python",
   "name": "django_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
